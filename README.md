<p align="center">
  <img src="https://img.shields.io/badge/SMILE-%F0%9F%A7%A0_Social_Memory_Integrated_Learning_Environment-blue?style=for-the-badge" alt="SMILE Logo">
</p>

<p align="center">
  <b>SMILE</b> â€” <i>Social Memory Integrated Learning Environment</i><br>
  <sub>AI-driven conversational system integrating face recognition, dialogue memory, and personalized user profiling.</sub>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.11-blue?logo=python&style=flat-square">
  <img src="https://img.shields.io/badge/Ollama-LLM-green?style=flat-square">
  <img src="https://img.shields.io/badge/DeepFace-Face_Recognition-orange?style=flat-square">
  <img src="https://img.shields.io/badge/Vosk-STT-yellow?style=flat-square">
  <img src="https://img.shields.io/badge/License-MIT-lightgrey?style=flat-square">
</p>

---

# ğŸ§  SMILE â€” *Social Memory Integrated Learning Environment*

**Version:** 1.0  
**Author:** Lorenzo (AI Robotics Lab)  
**Language:** Python 3.11  
**License:** MIT  

---

## ğŸ“˜ Overview

**SMILE** (*Social Memory Integrated Learning Environment*) is an intelligent conversational agent that integrates:
- **real-time face recognition**,  
- **context-aware dialogue**,  
- and **long-term personalized memory**.

The system recognizes users visually, maintains short-term conversation history, and automatically summarizes and updates each user's **long-term profile** through interaction.

It is built for **embodied AI** and **social robotics** contexts, where continuity and personalization in human-robot dialogue are key.

---

## ğŸ“¸ Core Features

| Feature | Description |
|----------|-------------|
| ğŸ§â€â™‚ï¸ **Face Recognition** | Detects and tracks multiple faces in real time via OpenCV and Facenet embeddings |
| ğŸ’¾ **Identity Memory** | Saves and reloads user embeddings with a persistent `.pkl` file |
| ğŸ—£ï¸ **Speech Interaction** | Records and transcribes voice using Vosk STT |
| ğŸ’¬ **Context-Aware Chat** | Uses Ollama (Llama 3 or any local model) to generate responses |
| ğŸ§  **Conversation History** | Each recognized person has a JSON log of past conversations |
| ğŸ”Š **Speech Synthesis** | Generates natural TTS output with `pyttsx3` |
| ğŸ§© **Thread-safe Concurrency** | Ensures only one active interaction per face |
| ğŸ’¡ **Configurable Silence Detection** | Dynamic voice/silence timing to improve conversational flow |

---

## ğŸ§° Tech Stack

- **Python 3.10+**
- **OpenCV** â€” face detection, tracking
- **Vosk** â€” offline STT engine
- **Ollama** â€” local LLM integration
- **pyttsx3** â€” TTS engine
- **Facenet / Dlib** â€” face embeddings
- **Threading & Async Queues** â€” concurrency handling

---

## ğŸ“¦ Project Structure

```
FaceRecognition/
â”‚
â”œâ”€â”€ data/ # Persistent user data
â”‚ â”œâ”€â”€ known_faces/ # Registered user images
â”‚ â”œâ”€â”€ conversations/ # Conversation transcripts
â”‚ â”œâ”€â”€ profiles/ # User profiles (JSON)
â”‚ â””â”€â”€ embeddings.pkl # Face embeddings database
â”‚
â”œâ”€â”€ src/ # Source code
â”‚ â”œâ”€â”€ config.py # Local configuration (ignored by Git)
â”‚ â”œâ”€â”€ recognize_live.py # Main live recognition and dialogue loop
â”‚ â”œâ”€â”€ utils/ # Functional modules
â”‚ â”‚ â”œâ”€â”€ dialog_manager.py
â”‚ â”‚ â”œâ”€â”€ memory_manager.py
â”‚ â”‚ â”œâ”€â”€ profile_manager.py
â”‚ â”‚ â”œâ”€â”€ speech_utils.py
â”‚ â”‚ â””â”€â”€ facenet_utils.py
â”‚ â””â”€â”€ init.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§© Core Modules

| Module | Description |
|---------|-------------|
| **recognize_live.py** | Main entry point. Handles video stream, voice input, and conversation logic. |
| **dialog_manager.py** | Builds prompts and manages dialogue state (GREETING, FREE_TALK, FAREWELL). |
| **memory_manager.py** | Maintains working and long-term memory, generates summaries via LLM. |
| **profile_manager.py** | Handles user profiles (creation, update, persistence). |
| **speech_utils.py** | Controls TTS and STT pipelines. |
| **facenet_utils.py** | Provides facial embedding and recognition functionality. |

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/<your-username>/FaceRecognition.git
cd FaceRecognition
```

### 2ï¸âƒ£ Create and Activate a Virtual Environment

```
python -m venv facenet
facenet\Scripts\activate   # Windows
# or
source facenet/bin/activate  # macOS/Linux
```

### 3ï¸âƒ£ Configure system and Install Dependencies

```
cp src/config_example.py src/config.py
pip install -r requirements.txt
```

### 4ï¸âƒ£ Download the Vosk Model (Italian)

Download from [Vosk Models](https://alphacephei.com/vosk/models) and extract it to a local folder outside the repo (to avoid huge commits). For example:

```
C:\Users\<you>\Documents\AI\models\vosk-model-it-0.22
```

Then update the model path in:

```
EXTERNAL_MODEL_DIR = r"C:\Users\<you>\Documents\AI\models\vosk-model-it-0.22"
```
---

## ğŸš€ How to Run

```
python -m src.recognize_live
```

Once running:

- The webcam feed will open.
- When a new face is detected, it will greet you and ask your name.
- It remembers you across sessions.
- You can speak freely â€” it listens, transcribes, responds, and speaks back.
- Conversations are saved per user in /data/conversations/.

---

## ğŸ’¾ Data Persistence

| File | Description |
|----------|-------------|
```data/embeddings.pkl``` | Stores facial embeddings and associated names |
| ```data/conversations/<user>.json``` | Conversation history for each recognized user |

---

## ğŸ§© Customization

- **Change language**  
  Replace the Vosk model path with another supported language model (e.g., English, Spanish, German).

- **Switch TTS voice**  
  Edit the `voices[0]` or `voices[1]` parameter in `speech_utils.py` to switch between male/female or different system voices.

- **Adjust silence detection**  
  Fine-tune `rms` thresholds, `silence_limit`, and `silence_hangover` inside the `transcribe_audio()` function for better sensitivity to pauses.

- **Change the AI model**  
  In `dialog_manager.py`, update the `"model": "llama3"` line to use a different Ollama model, such as `"mistral"`, `"llama3:instruct"`, or any locally available model.

- **Modify greetings or behavior**  
  The initial user greeting logic is defined inside `handle_interaction()` in `recognize_live.py`.  
  You can personalize how the assistant greets new or known users.

---

## ğŸ§  How Memory Works

Each recognized user has:
- A **face embedding** (a numeric vector representing their face)
- A **name**
- A **conversation history JSON file**

When the same person is detected again, the assistant automatically loads their identity and past interactions, resuming the context seamlessly. SMILE uses a hybrid memory system:

- **Short-term memory**: the last 7 conversational exchanges

- **Long-term memory**: summarized user profile stored in `data/profiles/`

Each session enriches the user profile with new insights extracted from conversation using an LLM-based summarization pipeline.

---

## ğŸ§ª Example Interaction

ğŸ§ New face detected!
- ğŸ¤–: "Hi there! I donâ€™t think weâ€™ve met before â€” whatâ€™s your name?"
- ğŸ‘¤: "Hi, Iâ€™m Lorenzo."
- ğŸ¤–: "Nice to meet you, Lorenzo! Iâ€™ll remember you from now on."
...
- ğŸ§  [Next session]
- ğŸ¤–: "Hey Lorenzo! Welcome back. How have you been?"

---

## âš ï¸ Notes

- Works best with clear audio input and good lighting conditions.
- Use a microphone configured for **16 kHz** or **48 kHz** sampling rate.
- Requires **Ollama** to be running locally (`ollama serve`).
- Do **not commit** the `models/` folder to GitHub, as the files are too large.

---

## ğŸ—ï¸ Future Improvements

| Milestone	| Status| Description |
|-----------|-------|-------------|
| v1.0 â€” SMILE Core	| âœ… Done | Real-time recognition, memory, summarization |
| v1.1 â€” Questionnaire Init	| ğŸ”œ Planned | Profile initialization for new users |
| v1.2 â€” Emotional Context | â³ In design | Affective state tracking & adaptive responses |
| v2.0 â€” Multi-agent Setup | âš™ï¸ Future	| Multi-person recognition and shared context |

---

## âš ï¸ Disclaimer

This system is intended for research and development in social robotics and conversational AI.
All personal data is stored locally and should be handled according to GDPR and privacy best practices.

---

## ğŸ§‘â€ğŸ’» Author

**Lorenzo D'Errico**  
PhD student in AI @ Federico II  
Email: [lorenzo.derrico@unina.it]  
LinkedIn: [linkedin/lo_de06]

---

## ğŸªª License

MIT License Â© 2025 â€” Developed by Lorenzo D'Errico