# ğŸ¤– FaceRecognition Assistant â€” Real-Time Vision & Voice AI

**FaceRecognition Assistant** is a multimodal AI system that combines:
- ğŸ¥ **Real-time face detection and recognition**
- ğŸ—£ï¸ **Speech-to-text (STT)** with [Vosk](https://alphacephei.com/vosk/)
- ğŸ’¬ **Conversational intelligence** powered by [Ollama](https://ollama.ai/)
- ğŸ”Š **Text-to-speech (TTS)** responses via `pyttsx3`
- ğŸ§  **Persistent memory** for user identity and conversation history

It can recognize you, remember previous interactions, and carry on contextual conversations â€” completely offline (for STT/TTS) and locally integrated with Ollama for reasoning.

---

## ğŸ“¸ Core Features

| Feature | Description |
|----------|-------------|
| ğŸ§â€â™‚ï¸ **Face Recognition** | Detects and tracks multiple faces in real time via OpenCV and Facenet embeddings |
| ğŸ’¾ **Identity Memory** | Saves and reloads user embeddings with a persistent `.pkl` file |
| ğŸ—£ï¸ **Speech Interaction** | Records and transcribes voice using Vosk STT |
| ğŸ’¬ **Context-Aware Chat** | Uses Ollama (Llama 3 or any local model) to generate responses |
| ğŸ§  **Conversation History** | Each recognized person has a JSON log of past conversations |
| ğŸ”Š **Speech Synthesis** | Generates natural TTS output with `pyttsx3` |
| ğŸ§© **Thread-safe Concurrency** | Ensures only one active interaction per face |
| ğŸ’¡ **Configurable Silence Detection** | Dynamic voice/silence timing to improve conversational flow |

---

## ğŸ§° Tech Stack

- **Python 3.10+**
- **OpenCV** â€” face detection, tracking
- **Vosk** â€” offline STT engine
- **Ollama** â€” local LLM integration
- **pyttsx3** â€” TTS engine
- **Facenet / Dlib** â€” face embeddings
- **Threading & Async Queues** â€” concurrency handling

---

## ğŸ“¦ Project Structure

```
FaceRecognition/
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ recognize_live.py # Main runtime script
â”‚ â”œâ”€â”€ utils/
â”‚ â”‚ â”œâ”€â”€ speech_utils.py # Speech recognition + TTS
â”‚ â”‚ â”œâ”€â”€ facenet_utils.py # Embedding & comparison logic
â”‚ â”‚ â”œâ”€â”€ dialog_manager.py # Ollama integration
â”‚ â”‚ â”œâ”€â”€ async_core.py # Thread pools, async queues
â”‚ â”‚ â””â”€â”€ ... # Other helpers
â”‚ â””â”€â”€ data/
â”‚ â”œâ”€â”€ embeddings.pkl # Stored face embeddings
â”‚ â””â”€â”€ conversations/ # JSON logs per user
â”‚
â”œâ”€â”€ models/
â”‚ â””â”€â”€ vosk-model-small-it-0.22/ # (Optional local copy)
â”‚
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/<your-username>/FaceRecognition.git
cd FaceRecognition
```

### 2ï¸âƒ£ Create and Activate a Virtual Environment

```
python -m venv facenet
facenet\Scripts\activate   # Windows
# or
source facenet/bin/activate  # macOS/Linux
```

### 3ï¸âƒ£ Install Dependencies

```
pip install -r requirements.txt
```

### 4ï¸âƒ£ Download the Vosk Model (Italian)

Download from [Vosk Models](https://alphacephei.com/vosk/models) and extract it to a local folder outside the repo (to avoid huge commits). For example:

```
C:\Users\<you>\Documents\AI\models\vosk-model-it-0.22
```

Then update the model path in:

```
EXTERNAL_MODEL_DIR = r"C:\Users\<you>\Documents\AI\models\vosk-model-it-0.22"
```
---

## ğŸš€ How to Run

```
cd src
python recognize_live.py
```

Once running:

- The webcam feed will open.
- When a new face is detected, it will greet you and ask your name.
- It remembers you across sessions.
- You can speak freely â€” it listens, transcribes, responds, and speaks back.
- Conversations are saved per user in /data/conversations/.

---

## ğŸ’¾ Data Persistence

| File | Description |
|----------|-------------|
```data/embeddings.pkl``` | Stores facial embeddings and associated names |
| ```data/conversations/<user>.json``` | Conversation history for each recognized user |

---

## ğŸ§© Customization

- **Change language**  
  Replace the Vosk model path with another supported language model (e.g., English, Spanish, German).

- **Switch TTS voice**  
  Edit the `voices[0]` or `voices[1]` parameter in `speech_utils.py` to switch between male/female or different system voices.

- **Adjust silence detection**  
  Fine-tune `rms` thresholds, `silence_limit`, and `silence_hangover` inside the `transcribe_audio()` function for better sensitivity to pauses.

- **Change the AI model**  
  In `dialog_manager.py`, update the `"model": "llama3"` line to use a different Ollama model, such as `"mistral"`, `"llama3:instruct"`, or any locally available model.

- **Modify greetings or behavior**  
  The initial user greeting logic is defined inside `handle_interaction()` in `recognize_live.py`.  
  You can personalize how the assistant greets new or known users.

---

## ğŸ§  How Memory Works

Each recognized user has:
- A **face embedding** (a numeric vector representing their face)
- A **name**
- A **conversation history JSON file**

When the same person is detected again, the assistant automatically loads their identity and past interactions, resuming the context seamlessly.

---

## ğŸ§ª Example Interaction

ğŸ§ New face detected!
- ğŸ¤–: "Hi there! I donâ€™t think weâ€™ve met before â€” whatâ€™s your name?"
- ğŸ‘¤: "Hi, Iâ€™m Lorenzo."
- ğŸ¤–: "Nice to meet you, Lorenzo! Iâ€™ll remember you from now on."
...
- ğŸ§  [Next session]
- ğŸ¤–: "Hey Lorenzo! Welcome back. How have you been?"

---

## âš ï¸ Notes

- Works best with clear audio input and good lighting conditions.
- Use a microphone configured for **16 kHz** or **48 kHz** sampling rate.
- Requires **Ollama** to be running locally (`ollama serve`).
- Do **not commit** the `models/` folder to GitHub, as the files are too large.

---

## ğŸ§‘â€ğŸ’» Author

**Lorenzo D'Errico**  
PhD student in AI @ Federico II  
Email: [lorenzo.derrico@unina.it]  
LinkedIn: [linkedin/lo_de06]

---

## ğŸ—ï¸ Future Improvements

- Improve TTS voice with neural synthesis  
- Add long-term, context-aware memory per user  
- Support emotional and gesture recognition  
- Handle multi-user group conversations  
- Optimize performance and UX flow

---

## ğŸªª License

MIT License Â© 2025 â€” Developed by Lorenzo D'Errico